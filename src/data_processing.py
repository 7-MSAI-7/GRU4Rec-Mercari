# -*- coding: utf-8 -*-
"""
ì´ íŒŒì¼ì€ ë°ì´í„° ì „ì²˜ë¦¬ì™€ ê´€ë ¨ëœ í•¨ìˆ˜ë“¤ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥ì€ ì›ë³¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ê³ ,
í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
"""
import os
import gc
import time
import pickle
import random
import pandas as pd
import torch
import src.settings as config
from tqdm import tqdm
import logging
from sentence_transformers import SentenceTransformer

# pandasì—ì„œ progress barë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
tqdm.pandas()
logger = logging.getLogger(__name__)

def load_and_preprocess_data_with_split(df_full, min_len_for_split):
    """
    ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ í•„ìš”í•œ ì •ë³´ë§Œ ê³¨ë¼ë‚´ê³ , ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•œ ë’¤,
    í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        df_full (pd.DataFrame): ì „ì²˜ë¦¬í•  ì „ì²´ ë°ì´í„°í”„ë ˆì„.
        min_len_for_split (int): í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ì›ë³¸ ì‹œí€€ìŠ¤ ê¸¸ì´.

    Returns:
        tuple: (í•™ìŠµ ìƒ˜í”Œ, ê²€ì¦ ìƒ˜í”Œ, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ), ê°ì¢… ë§µí•‘ ì‚¬ì „, ì•„ì´í…œ ì •ë³´ ë°ì´í„°í”„ë ˆì„ ë“±ì„ í¬í•¨í•˜ëŠ” íŠœí”Œ.
    """
    # SentenceTransformer ëª¨ë¸ ë¡œë“œ: í…ìŠ¤íŠ¸(ìƒí’ˆëª…)ë¥¼ ìˆ«ì ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    sentence_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    start_time = time.time()
    logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘... ğŸšš (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”!)")

    # ì„¤ì •ì— ë”°ë¼ ì¼ë¶€ ë°ì´í„°ë§Œ ë¶ˆëŸ¬ì˜¬ì§€ ê²°ì •í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ìš©).
    if config.N_ROWS_TO_LOAD:
        df_full = df_full[:config.N_ROWS_TO_LOAD]

    # --- 1. ê¸°ë³¸ ë°ì´í„° ì •ë¦¬ ---
    # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ë°ì´í„°ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    df_full = df_full[df_full["sequence_length"] >= min_len_for_split]
    # ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì„ ì»¬ëŸ¼ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    df_full = df_full.drop(
        columns=[
            "c0_id", "c1_id", "c2_id", "shipper_name", "shipper_id", 
            "sequence_length", "item_condition_id", "item_condition_name", 
            "size_id", "size_name", "brand_id", "brand_name", 
            "color", "price", "product_id"
        ],
        errors='ignore'  # í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ì–´ë„ ì˜¤ë¥˜ë¥¼ ë‚´ì§€ ì•ŠìŒ
    )
    
    logger.info("ì „ì²˜ë¦¬ í›„ ë°ì´í„° í™•ì¸...")
    logger.info(f"ë°ì´í„° í˜•íƒœ: {df_full.shape}")
    logger.info(f"ì»¬ëŸ¼: {df_full.columns.tolist()}")

    # 'event_id' ì»¬ëŸ¼ì„ 'category' íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    df_full["event_id"] = df_full["event_id"].astype("category")
    logger.info(
        f"'event_id'ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í–ˆì–´ìš”. ê³ ìœ í•œ í–‰ë™ ì¢…ë¥˜ëŠ” {len(df_full['event_id'].cat.categories)}ê°€ì§€ ì…ë‹ˆë‹¤."
    )

    # í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    train_samples, valid_samples, test_samples = [], [], []

    # --- 2. ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ ê·¸ë£¹í™” ---
    # ë°ì´í„°ë¥¼ ì‚¬ìš©ì, ì„¸ì…˜, ì‹œí€€ìŠ¤ ID ë° ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    df_full_sorted = df_full.sort_values(
        by=["user_id", "session_id", "sequence_id", "stime"]
    )
    # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë³„ì ì¸ ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤(ì—¬ì •)ë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    grouped_by_sequence = df_full_sorted.groupby(
        ["user_id", "session_id", "sequence_id"]
    )
    logger.info(
        f"ì´ {len(grouped_by_sequence)}ê°œì˜ ì›ë³¸ ì‡¼í•‘ ì—¬ì •(ì‹œí€€ìŠ¤) ë¬¶ìŒì„ ì°¾ì•˜ì–´ìš”. ì´ì œ ë‚˜ëˆ ë³¼ê²Œìš”..."
    )

    # --- 3. ì•„ì´í…œ ë° ì´ë²¤íŠ¸ ID ë§µí•‘ ìƒì„± ---
    # ê° ì•„ì´í…œ IDë¥¼ ê³ ìœ í•œ ìˆ«ì(ì¸ë±ìŠ¤)ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‚¬ì „ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    # "<PAD_ITEM_ID>"ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ ê°€ìƒì˜ ì•„ì´í…œì…ë‹ˆë‹¤.
    item_id_to_idx = {"<PAD_ITEM_ID>": 0}
    # ì´ì „ì— ë§Œë“¤ì–´ë‘” ë§µí•‘ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if os.path.exists(config.ITEM_ID_IDX_PATH):
        with open(config.ITEM_ID_IDX_PATH, "rb") as f:
            item_id_to_idx = pickle.load(f)

    # í˜„ì¬ ë°ì´í„°ì—ë§Œ ìˆëŠ” ìƒˆë¡œìš´ ì•„ì´í…œë“¤ì„ ì°¾ì•„ ë§µí•‘ ì‚¬ì „ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    unique_item_ids = [item_id for item_id in df_full["item_id"].unique().tolist() if item_id not in item_id_to_idx.keys()]
    new_item_id_to_idx = {item_id: len(item_id_to_idx) + i for i, item_id in enumerate(unique_item_ids)}
    
    # ìƒˆë¡œìš´ ì•„ì´í…œ ì •ë³´ë¥¼ ê¸°ì¡´ ì‚¬ì „ì— ì—…ë°ì´íŠ¸í•˜ê³ , íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    item_id_to_idx.update(new_item_id_to_idx)
    with open(config.ITEM_ID_IDX_PATH, "wb") as f:
        pickle.dump(item_id_to_idx, f)

    # ì¸ë±ìŠ¤ì—ì„œ ì•„ì´í…œ IDë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì—­ë°©í–¥ ë§µí•‘ì„ ë§Œë“­ë‹ˆë‹¤.
    idx_to_item_id = {v: k for k, v in item_id_to_idx.items()}
    logger.info(f"ì´ ì•„ì´í…œ ê°œìˆ˜ (íŒ¨ë”© í¬í•¨): {len(item_id_to_idx)}")

    # ê° ì´ë²¤íŠ¸(í–‰ë™)ë¥¼ ê³ ìœ í•œ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‚¬ì „ì„ ì •ì˜í•©ë‹ˆë‹¤.
    event_to_idx = {
        "<PAD_EVENT>": 0, "item_view": 1, "item_like": 2, "item_add_to_cart_tap": 3,
        "offer_make": 4, "buy_start": 5, "buy_comp": 6,
    }
    logger.info(f"ì´ ì´ë²¤íŠ¸ ê°œìˆ˜ (íŒ¨ë”© í¬í•¨): {len(event_to_idx)}")
    
    # --- 4. ì•„ì´í…œ ì´ë¦„ ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ ---
    # ì•„ì´í…œ ì¸ë±ìŠ¤ë¥¼ í•´ë‹¹ ì•„ì´í…œì˜ ì´ë¦„ ì„ë² ë”©(ìˆ«ì ë²¡í„°)ì— ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    item_idx_to_embedded_name = {}
    # ì´ì „ì— ìƒì„±í•œ ì„ë² ë”© íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    if os.path.exists(config.ITEM_IDX_NAME_PATH):
        logger.info(f"ê¸°ì¡´ ì„ë² ë”© íŒŒì¼ ë¡œë“œ ë° ì—…ë°ì´íŠ¸: {config.ITEM_IDX_NAME_PATH}")
        loaded_data = torch.load(config.ITEM_IDX_NAME_PATH, map_location='cpu', weights_only=False)
        keys = loaded_data["keys"]
        tensors = loaded_data["tensors"]
        item_idx_to_embedded_name = {key: tensors[i] for i, key in enumerate(keys)}
    
    # ë°ì´í„°ì—ì„œ ê³ ìœ í•œ ì•„ì´í…œ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    unique_items_df = df_full[['item_id', 'name']].drop_duplicates(subset=['item_id'])
    
    # ì•„ì§ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ì•„ì´í…œë“¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    unique_items_df['idx'] = unique_items_df['item_id'].map(item_id_to_idx)
    new_items_df = unique_items_df[~unique_items_df['idx'].isin(item_idx_to_embedded_name.keys())]
    
    # ìƒˆë¡œìš´ ì•„ì´í…œì´ ìˆë‹¤ë©´, ì´ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    if not new_items_df.empty:
        new_items_to_embed = pd.Series(new_items_df['name'].values, index=new_items_df['idx']).to_dict()
        logger.info(f"{len(new_items_to_embed)}ê°œì˜ ìƒˆë¡œìš´ ì•„ì´í…œì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        new_indices = list(new_items_to_embed.keys())
        new_names = list(new_items_to_embed.values())
        
        # SentenceTransformer ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¦„(í…ìŠ¤íŠ¸)ì„ ì„ë² ë”©(ìˆ«ì ë²¡í„°)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        new_embeddings = sentence_model.encode(
            new_names,
            show_progress_bar=True,
            convert_to_tensor=True,
        )
        
        # ìƒˆë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        item_idx_to_embedded_name.update(dict(zip(new_indices, new_embeddings)))

        keys_list = list(item_idx_to_embedded_name.keys())
        tensor_list = list(item_idx_to_embedded_name.values())
        data_to_save = {"keys": keys_list, "tensors": tensor_list}
        torch.save(data_to_save, config.ITEM_IDX_NAME_PATH)
        logger.info("ì„ë² ë”© íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    # --- 5. ì‹œí€€ìŠ¤ ë¶„í•  ë° í•™ìŠµ ë°ì´í„° ìƒì„± ---
    # ê·¸ë£¹í™”ëœ ê° ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ë°˜ë³µ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    for _, group in tqdm(grouped_by_sequence, desc="ì‹œí€€ìŠ¤ ë¶„í•  ë° ìƒ˜í”Œë§"):
        item_ids = group["item_id"].tolist()
        
        # í˜„ì¬ ì‹œí€€ìŠ¤ì— í¬í•¨ëœ ì•„ì´í…œë“¤ì˜ ì´ë¦„ ì„ë² ë”©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        name_sequences = [
            item_idx_to_embedded_name.get(item_id_to_idx.get(item_id))
            for item_id in item_ids
        ]
        
        # í˜„ì¬ ì‹œí€€ìŠ¤ì— í¬í•¨ëœ ì´ë²¤íŠ¸ë“¤ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        event_sequences = [
            event_to_idx.get(e, 0) for e in group["event_id"].tolist()
        ]

        # í˜„ì¬ ì‹œí€€ìŠ¤ì— í¬í•¨ëœ ì•„ì´í…œë“¤ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        item_idx_sequences = [
            item_id_to_idx[item_id] for item_id in item_ids
        ]

        # (ì´ë¦„ ì„ë² ë”©, ì´ë²¤íŠ¸ ì¸ë±ìŠ¤, ì•„ì´í…œ ì¸ë±ìŠ¤)ë¥¼ í•˜ë‚˜ì˜ íŠœí”Œë¡œ ë¬¶ìŠµë‹ˆë‹¤.
        current_paired_sequences = list(
            zip(name_sequences, event_sequences, item_idx_sequences)
        )

        # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê¸° ë¶€ì í•©í•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
        if len(current_paired_sequences) < config.MIN_LEN_FOR_SEQ_SPLIT:
            continue

        # --- ë°ì´í„° ë¶„í• : Leave-One-Out ë°©ì‹ ---
        # ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ í–‰ë™ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
        # ì˜ˆ: ì‹œí€€ìŠ¤ [A, B, C, D]
        # Test:  Input: [A, B, C] -> Target: D (ê°€ì¥ ë§ˆì§€ë§‰ í–‰ë™ ì˜ˆì¸¡)
        # Valid: Input: [A, B]    -> Target: C (ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ í–‰ë™ ì˜ˆì¸¡)
        # Train: Input: [A]       -> Target: B (ê·¸ ì´ì „ì˜ ëª¨ë“  ìƒí˜¸ì‘ìš©ì„ í•™ìŠµ)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_input_sequences = current_paired_sequences[:-1]  # ë§ˆì§€ë§‰ ì•„ì´í…œì„ ì œì™¸í•œ ëª¨ë“  ì‹œí€€ìŠ¤
        test_target_item_idx = current_paired_sequences[-1][2]  # ë§ˆì§€ë§‰ ì•„ì´í…œì˜ ì¸ë±ìŠ¤ê°€ ì •ë‹µ
        if test_input_sequences:
            test_samples.append(
                (
                    (
                        [s[0] for s in test_input_sequences], # ì´ë¦„ ì„ë² ë”© ì‹œí€€ìŠ¤
                        [s[1] for s in test_input_sequences], # ì´ë²¤íŠ¸ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤
                    ),
                    test_target_item_idx, # ì •ë‹µ ì•„ì´í…œ ì¸ë±ìŠ¤
                )
            )

        # ê²€ì¦ ë°ì´í„° ìƒì„±
        valid_input_sequences = current_paired_sequences[:-2] # ë§ˆì§€ë§‰ ë‘ ê°œ ì•„ì´í…œì„ ì œì™¸í•œ ì‹œí€€ìŠ¤
        valid_target_item_idx = current_paired_sequences[-2][2] # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ì•„ì´í…œì´ ì •ë‹µ
        if valid_input_sequences:
            valid_samples.append(
                (
                    (
                        [s[0] for s in valid_input_sequences],
                        [s[1] for s in valid_input_sequences],
                    ),
                    valid_target_item_idx,
                )
            )

        # í•™ìŠµ ë°ì´í„° ìƒì„±
        # ì‹œí€€ìŠ¤ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ìƒ˜í”Œì„ ë§Œë“­ë‹ˆë‹¤.
        # ì˜ˆ: [A,B,C]ê°€ ìˆë‹¤ë©´, ([A])->B, ([A,B])->C ë¥¼ ë‘ ê°œì˜ í•™ìŠµ ìƒ˜í”Œë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        train_sequences = current_paired_sequences[:-2]
        for i in range(1, len(train_sequences)):
            train_input = train_sequences[:i]
            train_target = train_sequences[i][2]
            train_samples.append(
                (
                    (
                        [s[0] for s in train_input],
                        [s[1] for s in train_input],
                    ),
                    train_target,
                )
            )

    # --- 6. ìµœì¢… ì •ë¦¬ ---
    # ì¶”ì²œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ ë•Œ ì‚¬ìš©í•  ì•„ì´í…œ ì •ë³´ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“­ë‹ˆë‹¤.
    df_item_info = None
    if "item_id" in df_full.columns and "name" in df_full.columns:
        df_item_info = (
            df_full[["item_id", "name", "c0_name", "c1_name", "c2_name"]]
                .drop_duplicates(subset=["item_id"])
                .set_index("item_id")
        )
        logger.info(f"ì¶”ì²œ ê²°ê³¼ì— í‘œì‹œí•  ì•„ì´í…œ ì´ë¦„ ì •ë³´ {len(df_item_info)}ê°œë¥¼ ì¤€ë¹„í–ˆì–´ìš”.")

    logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train {len(train_samples)}, Valid {len(valid_samples)}, Test {len(test_samples)} ìƒ˜í”Œ")
    
    end_time = time.time()
    logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {end_time - start_time:.2f}ì´ˆ ê±¸ë ¸ì–´ìš”.")

    # ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ë” ì´ìƒ í•„ìš” ì—†ëŠ” ëª¨ë¸ ê°ì²´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    del sentence_model
    gc.collect()

    # ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return (
        train_samples, valid_samples, test_samples,
        item_id_to_idx, event_to_idx, idx_to_item_id,
        item_idx_to_embedded_name, df_item_info
    ) 