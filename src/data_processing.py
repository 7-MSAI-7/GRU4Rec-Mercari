# -*- coding: utf-8 -*-
"""
ì´ íŒŒì¼ì€ ë°ì´í„° ì „ì²˜ë¦¬ì™€ ê´€ë ¨ëœ í•¨ìˆ˜ë“¤ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥ì€ ì›ë³¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ê³ ,
í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
"""
import os
import gc
import time
import pickle
import random
import pandas as pd
import torch
import src.settings as config
from tqdm import tqdm
import logging
from sentence_transformers import SentenceTransformer

# pandasì—ì„œ progress barë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
tqdm.pandas()
logger = logging.getLogger(__name__)

def load_and_preprocess_data_with_split(df_full, min_len_for_split):
    """
    ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ í•„ìš”í•œ ì •ë³´ë§Œ ê³¨ë¼ë‚´ê³ , ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•œ ë’¤,
    í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        df_full (pd.DataFrame): ì „ì²˜ë¦¬í•  ì „ì²´ ë°ì´í„°í”„ë ˆì„.
        min_len_for_split (int): í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ì›ë³¸ ì‹œí€€ìŠ¤ ê¸¸ì´.

    Returns:
        tuple: (í•™ìŠµ ìƒ˜í”Œ, ê²€ì¦ ìƒ˜í”Œ, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ), ê°ì¢… ë§µí•‘ ì‚¬ì „, ì•„ì´í…œ ì •ë³´ ë°ì´í„°í”„ë ˆì„ ë“±ì„ í¬í•¨í•˜ëŠ” íŠœí”Œ.
    """
    # SentenceTransformer ëª¨ë¸ ë¡œë“œ: í…ìŠ¤íŠ¸(ìƒí’ˆëª…)ë¥¼ ìˆ«ì ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    sentence_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    start_time = time.time()
    logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘... ğŸšš (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”!)")

    # ì„¤ì •ì— ë”°ë¼ ì¼ë¶€ ë°ì´í„°ë§Œ ë¶ˆëŸ¬ì˜¬ì§€ ê²°ì •í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ìš©).
    if config.N_ROWS_TO_LOAD:
        df_full = df_full[:config.N_ROWS_TO_LOAD]

    # --- 1. ê¸°ë³¸ ë°ì´í„° ì •ë¦¬ ---
    # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ë°ì´í„°ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    df_full = df_full[df_full["sequence_length"] >= min_len_for_split]
    # ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì„ ì»¬ëŸ¼ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    df_full = df_full.drop(
        columns=[
            "c0_id", "c1_id", "c2_id", "shipper_name", "shipper_id", 
            "sequence_length", "item_condition_id", "item_condition_name", 
            "size_id", "size_name", "brand_id", "brand_name", 
            "color", "price", "product_id"
        ],
        errors='ignore'  # í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ì–´ë„ ì˜¤ë¥˜ë¥¼ ë‚´ì§€ ì•ŠìŒ
    )
    
    logger.info("ì „ì²˜ë¦¬ í›„ ë°ì´í„° í™•ì¸...")
    logger.info(f"ë°ì´í„° í˜•íƒœ: {df_full.shape}")
    logger.info(f"ì»¬ëŸ¼: {df_full.columns.tolist()}")

    # 'event_id' ì»¬ëŸ¼ì„ 'category' íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    df_full["event_id"] = df_full["event_id"].astype("category")
    logger.info(
        f"'event_id'ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í–ˆì–´ìš”. ê³ ìœ í•œ í–‰ë™ ì¢…ë¥˜ëŠ” {len(df_full['event_id'].cat.categories)}ê°€ì§€ ì…ë‹ˆë‹¤."
    )

    # í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    train_samples, valid_samples, test_samples = [], [], []

    # --- 2. ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ ê·¸ë£¹í™” ---
    # ë°ì´í„°ë¥¼ ì‚¬ìš©ì, ì„¸ì…˜, ì‹œí€€ìŠ¤ ID ë° ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    df_full_sorted = df_full.sort_values(
        by=["user_id", "session_id", "sequence_id", "stime"]
    )
    # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë³„ì ì¸ ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤(ì—¬ì •)ë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    grouped_by_sequence = df_full_sorted.groupby(
        ["user_id", "session_id", "sequence_id"]
    )
    logger.info(
        f"ì´ {len(grouped_by_sequence)}ê°œì˜ ì›ë³¸ ì‡¼í•‘ ì—¬ì •(ì‹œí€€ìŠ¤) ë¬¶ìŒì„ ì°¾ì•˜ì–´ìš”. ì´ì œ ë‚˜ëˆ ë³¼ê²Œìš”..."
    )

    # --- 3. ì•„ì´í…œ ë° ì´ë²¤íŠ¸ ID ë§µí•‘ ìƒì„± ---
    # ê° ì•„ì´í…œ IDë¥¼ ê³ ìœ í•œ ìˆ«ì(ì¸ë±ìŠ¤)ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‚¬ì „ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    # "<PAD_ITEM_ID>"ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ ê°€ìƒì˜ ì•„ì´í…œì…ë‹ˆë‹¤.
    item_id_to_idx = {"<PAD_ITEM_ID>": 0}
    # ì´ì „ì— ë§Œë“¤ì–´ë‘” ë§µí•‘ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if os.path.exists(config.ITEM_ID_IDX_PATH):
        with open(config.ITEM_ID_IDX_PATH, "rb") as f:
            item_id_to_idx = pickle.load(f)

    # í˜„ì¬ ë°ì´í„°ì—ë§Œ ìˆëŠ” ìƒˆë¡œìš´ ì•„ì´í…œë“¤ì„ ì°¾ì•„ ë§µí•‘ ì‚¬ì „ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    unique_item_ids = [item_id for item_id in df_full["item_id"].unique().tolist() if item_id not in item_id_to_idx.keys()]
    new_item_id_to_idx = {item_id: len(item_id_to_idx) + i for i, item_id in enumerate(unique_item_ids)}
    
    # ìƒˆë¡œìš´ ì•„ì´í…œ ì •ë³´ë¥¼ ê¸°ì¡´ ì‚¬ì „ì— ì—…ë°ì´íŠ¸í•˜ê³ , íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    item_id_to_idx.update(new_item_id_to_idx)
    with open(config.ITEM_ID_IDX_PATH, "wb") as f:
        pickle.dump(item_id_to_idx, f)

    # ì¸ë±ìŠ¤ì—ì„œ ì•„ì´í…œ IDë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì—­ë°©í–¥ ë§µí•‘ì„ ë§Œë“­ë‹ˆë‹¤.
    idx_to_item_id = {v: k for k, v in item_id_to_idx.items()}
    logger.info(f"ì´ ì•„ì´í…œ ê°œìˆ˜ (íŒ¨ë”© í¬í•¨): {len(item_id_to_idx)}")

    # ê° ì´ë²¤íŠ¸(í–‰ë™)ë¥¼ ê³ ìœ í•œ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‚¬ì „ì„ ì •ì˜í•©ë‹ˆë‹¤.
    event_to_idx = {
        "<PAD_EVENT>": 0, "item_view": 1, "item_like": 2, "item_add_to_cart_tap": 3,
        "offer_make": 4, "buy_start": 5, "buy_comp": 6,
    }
    logger.info(f"ì´ ì´ë²¤íŠ¸ ê°œìˆ˜ (íŒ¨ë”© í¬í•¨): {len(event_to_idx)}")
    
    # --- 4. ì•„ì´í…œ ì´ë¦„ ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ ---
    # ì•„ì´í…œ ì¸ë±ìŠ¤ë¥¼ í•´ë‹¹ ì•„ì´í…œì˜ ì´ë¦„ ì„ë² ë”©(ìˆ«ì ë²¡í„°)ì— ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    item_idx_to_embedded_name = {}
    # ì´ì „ì— ìƒì„±í•œ ì„ë² ë”© íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    if os.path.exists(config.ITEM_IDX_NAME_PATH):
        logger.info(f"ê¸°ì¡´ ì„ë² ë”© íŒŒì¼ ë¡œë“œ ë° ì—…ë°ì´íŠ¸: {config.ITEM_IDX_NAME_PATH}")
        loaded_data = torch.load(config.ITEM_IDX_NAME_PATH, map_location='cpu', weights_only=False)
        keys = loaded_data["keys"]
        tensors = loaded_data["tensors"]
        item_idx_to_embedded_name = {key: tensors[i] for i, key in enumerate(keys)}
    
    # ë°ì´í„°ì—ì„œ ê³ ìœ í•œ ì•„ì´í…œ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    unique_items_df = df_full[['item_id', 'name']].drop_duplicates(subset=['item_id'])
    
    # ì•„ì§ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ì•„ì´í…œë“¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    unique_items_df['idx'] = unique_items_df['item_id'].map(item_id_to_idx)
    new_items_df = unique_items_df[~unique_items_df['idx'].isin(item_idx_to_embedded_name.keys())]
    
    # ìƒˆë¡œìš´ ì•„ì´í…œì´ ìˆë‹¤ë©´, ì´ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    if not new_items_df.empty:
        new_items_to_embed = pd.Series(new_items_df['name'].values, index=new_items_df['idx']).to_dict()
        logger.info(f"{len(new_items_to_embed)}ê°œì˜ ìƒˆë¡œìš´ ì•„ì´í…œì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        new_indices = list(new_items_to_embed.keys())
        new_names = list(new_items_to_embed.values())
        
        # SentenceTransformer ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¦„(í…ìŠ¤íŠ¸)ì„ ì„ë² ë”©(ìˆ«ì ë²¡í„°)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        new_embeddings = sentence_model.encode(
            new_names,
            show_progress_bar=True,
            convert_to_tensor=True,
        )
        
        # ìƒˆë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        item_idx_to_embedded_name.update(dict(zip(new_indices, new_embeddings)))

        keys_list = list(item_idx_to_embedded_name.keys())
        tensor_list = list(item_idx_to_embedded_name.values())
        data_to_save = {"keys": keys_list, "tensors": tensor_list}
        torch.save(data_to_save, config.ITEM_IDX_NAME_PATH)
        logger.info("ì„ë² ë”© íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    # --- 4.5. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì¹´í…Œê³ ë¦¬ ë¶ˆê· í˜• í•´ì†Œ) ---
    logger.info("ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬(c0_name) ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ ì†ì‹¤ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹œì‘...")
    
    class_weights = None
    if 'c0_name' in df_full.columns:
        # 1. ì•„ì´í…œë³„ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë§¤í•‘ ìƒì„±
        # drop_duplicates: ì•„ì´í…œ IDë‹¹ í•˜ë‚˜ì˜ c0_nameë§Œ ìˆë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. (ë°ì´í„°ì— ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° ëŒ€ë¹„)
        item_id_to_c0_name = df_full[['item_id', 'c0_name']].drop_duplicates(subset=['item_id']).set_index('item_id')['c0_name']

        # 2. ì¹´í…Œê³ ë¦¬ë³„ ë¹ˆë„ìˆ˜ ê³„ì‚°
        # ì „ì²´ ë°ì´í„°ì…‹ì˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        category_counts = df_full['c0_name'].value_counts()
        
        # 3. ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ê³„ì‚° (Inverse Frequency Weighting)
        # ê³µì‹: weight = ì „ì²´ ìƒ˜í”Œ ìˆ˜ / (í´ë˜ìŠ¤ ìˆ˜ * í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜)
        # ì´ë¥¼ í†µí•´ ì†Œìˆ˜ í´ë˜ìŠ¤(ë¹ˆë„ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
        total_samples = category_counts.sum()
        num_categories = len(category_counts)
        
        # category_countsê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
        if num_categories > 0:
            category_weight_map = (total_samples / (num_categories * category_counts)).to_dict()
        else:
            category_weight_map = {}

        # 4. ì•„ì´í…œ ì¸ë±ìŠ¤ë³„ ê°€ì¤‘ì¹˜ í…ì„œ ìƒì„±
        num_items = len(item_id_to_idx)
        class_weights_list = [1.0] * num_items # ê¸°ë³¸ ê°€ì¤‘ì¹˜ëŠ” 1ë¡œ ì„¤ì •
        
        for item_id, idx in item_id_to_idx.items():
            if item_id == "<PAD_ITEM_ID>":
                class_weights_list[idx] = 0.0 # PAD í† í°ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
                continue
            
            # ì•„ì´í…œ IDì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ ì°¾ê³ , ê·¸ ì¹´í…Œê³ ë¦¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹
            c0_name = item_id_to_c0_name.get(item_id)
            if c0_name and c0_name in category_weight_map:
                class_weights_list[idx] = category_weight_map[c0_name]

        class_weights = torch.FloatTensor(class_weights_list)
        
        logger.info("ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ.")
        if category_weight_map:
            logger.info(f"ê³„ì‚°ëœ ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜ (ìƒìœ„ 5ê°œ ìƒ˜í”Œ): { {k: v for k, v in list(category_weight_map.items())[:5]} }")
    else:
        logger.warning("'c0_name' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì•„ì´í…œ ê°€ì¤‘ì¹˜ë¥¼ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        num_items = len(item_id_to_idx)
        class_weights = torch.ones(num_items)
        class_weights[item_id_to_idx["<PAD_ITEM_ID>"]] = 0.0

    # --- 5. ì‹œí€€ìŠ¤ ë¶„í•  ë° í•™ìŠµ ë°ì´í„° ìƒì„± ---
    logger.info("ì‚¬ìš©ì(ì„¸ì…˜) ê¸°ë°˜ìœ¼ë¡œ Train/Valid/Test ë°ì´í„° ë¶„í•  ì‹œì‘...")
    
    # ì „ì²´ ì‹œí€€ìŠ¤ ê·¸ë£¹ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    all_sequences = list(grouped_by_sequence)
    # ì¬ìƒì‚°ì„±ì„ ìœ„í•´ ëœë¤ ì‹œë“œ ê³ ì •
    random.seed(42)
    # ê·¸ë£¹ì„ ë¬´ì‘ìœ„ë¡œ ì„ìŒ
    random.shuffle(all_sequences)

    # ë°ì´í„°ì…‹ í¬ê¸° ê³„ì‚°
    total_size = len(all_sequences)
    train_size = int(total_size * 0.7)
    valid_size = int(total_size * 0.2)
    
    # ê·¸ë£¹ ë¶„í• 
    train_groups = all_sequences[:train_size]
    valid_groups = all_sequences[train_size : train_size + valid_size]
    test_groups = all_sequences[train_size + valid_size :]

    logger.info(f"ë°ì´í„° ê·¸ë£¹ ë¶„í•  ì™„ë£Œ: Train: {len(train_groups)}, Valid: {len(valid_groups)}, Test: {len(test_groups)} ê·¸ë£¹")

    def create_samples_from_groups(groups, dataset_type):
        """
        ì£¼ì–´ì§„ ê·¸ë£¹ìœ¼ë¡œë¶€í„° í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
        Leave-one-out ë°©ì‹ì„ ê° ê·¸ë£¹ ë‚´ì—ì„œë§Œ ì ìš©.
        """
        samples = []
        for _, group in tqdm(groups, desc=f"{dataset_type} ìƒ˜í”Œ ìƒì„± ì¤‘"):
            item_ids = group["item_id"].tolist()
            
            name_sequences = [
                item_idx_to_embedded_name.get(item_id_to_idx.get(item_id))
                for item_id in item_ids
            ]
            
            event_sequences = [
                event_to_idx.get(e, 0) for e in group["event_id"].tolist()
            ]

            item_idx_sequences = [
                item_id_to_idx[item_id] for item_id in item_ids
            ]

            current_paired_sequences = list(
                zip(name_sequences, event_sequences, item_idx_sequences)
            )

            if len(current_paired_sequences) < config.MIN_LEN_FOR_SEQ_SPLIT:
                continue

            # --- Leave-One-Out ìƒ˜í”Œë§ ---
            # ê° ì‚¬ìš©ì ì‹œí€€ìŠ¤ ë‚´ì—ì„œ, ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
            # ì´ ë°©ì‹ì€ ì´ì œ ê° ë¶„í• ëœ ë°ì´í„°ì…‹(Train/Valid/Test) ê·¸ë£¹ ë‚´ì—ì„œë§Œ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.
            # ì˜ˆì‹œ ì‹œí€€ìŠ¤: [A, B, C, D, E]
            if dataset_type == 'test':
                # í…ŒìŠ¤íŠ¸ì…‹: ì‚¬ìš©ìì˜ ê°€ì¥ ë§ˆì§€ë§‰ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
                # Input: [A, B, C, D] -> Target: E
                input_sequences = current_paired_sequences[:-1]
                target_item_idx = current_paired_sequences[-1][2]
                if input_sequences:
                    samples.append(
                        (
                            (
                                [s[0] for s in input_sequences],
                                [s[1] for s in input_sequences],
                            ),
                            target_item_idx,
                        )
                    )
            elif dataset_type == 'valid':
                # ê²€ì¦ì…‹: ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
                # ì´ë¥¼ í†µí•´ í•™ìŠµ ì¤‘ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
                # Input: [A, B, C] -> Target: D
                input_sequences = current_paired_sequences[:-2]
                target_item_idx = current_paired_sequences[-2][2]
                if input_sequences:
                    samples.append(
                        (
                            (
                                [s[0] for s in input_sequences],
                                [s[1] for s in input_sequences],
                            ),
                            target_item_idx,
                        )
                    )
            else: # train
                # í•™ìŠµì…‹: ì‹œí€€ìŠ¤ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° ì—¬ëŸ¬ í•™ìŠµ ìƒ˜í”Œì„ ë§Œë“­ë‹ˆë‹¤.
                # ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì‹œí€€ìŠ¤ì˜ ë‹¤ìŒ ì•„ì´í…œì„ ì˜ˆì¸¡í•˜ëŠ” íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
                # ì˜ˆì‹œ:
                # Input: [A]       -> Target: B
                # Input: [A, B]    -> Target: C
                train_sequences_all = current_paired_sequences[:-2]
                for i in range(1, len(train_sequences_all)):
                    train_input = train_sequences_all[:i]
                    train_target = train_sequences_all[i][2]
                    if train_input:
                        samples.append(
                            (
                                (
                                    [s[0] for s in train_input],
                                    [s[1] for s in train_input],
                                ),
                                train_target,
                            )
                        )
        return samples

    # ê° ê·¸ë£¹ìœ¼ë¡œë¶€í„° ìƒ˜í”Œ ìƒì„±
    train_samples = create_samples_from_groups(train_groups, 'train')
    valid_samples = create_samples_from_groups(valid_groups, 'valid')
    test_samples = create_samples_from_groups(test_groups, 'test')

    # --- 6. ìµœì¢… ì •ë¦¬ ---
    # ì¶”ì²œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ ë•Œ ì‚¬ìš©í•  ì•„ì´í…œ ì •ë³´ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“­ë‹ˆë‹¤.
    df_item_info = None
    if "item_id" in df_full.columns and "name" in df_full.columns:
        df_item_info = (
            df_full[["item_id", "name", "c0_name", "c1_name", "c2_name"]]
                .drop_duplicates(subset=["item_id"])
                .set_index("item_id")
        )
        logger.info(f"ì¶”ì²œ ê²°ê³¼ì— í‘œì‹œí•  ì•„ì´í…œ ì´ë¦„ ì •ë³´ {len(df_item_info)}ê°œë¥¼ ì¤€ë¹„í–ˆì–´ìš”.")

    logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train {len(train_samples)}, Valid {len(valid_samples)}, Test {len(test_samples)} ìƒ˜í”Œ")
    
    end_time = time.time()
    logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {end_time - start_time:.2f}ì´ˆ ê±¸ë ¸ì–´ìš”.")

    # ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ë” ì´ìƒ í•„ìš” ì—†ëŠ” ëª¨ë¸ ê°ì²´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    del sentence_model
    gc.collect()

    # ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return (
        train_samples, valid_samples, test_samples,
        item_id_to_idx, event_to_idx, idx_to_item_id,
        item_idx_to_embedded_name, df_item_info, class_weights
    ) 